---
- name: Validate required deployment variables are provided
  ansible.builtin.assert:
    that:
      - dockerhub_username is defined
      - dockerhub_username | length > 0
      - dockerhub_password is defined
      - dockerhub_password | length > 0
      - account_service_image_repo is defined
      - account_service_image_repo | length > 0
      - transaction_service_image_repo is defined
      - transaction_service_image_repo | length > 0
      - canary_tag is defined
      - canary_tag | length > 0
      - (canary_traffic_percentage | int) >= 0
      - (canary_traffic_percentage | int) <= 100
    fail_msg: >
      Docker Hub credentials, image repositories, and canary tag must be supplied via extra-vars or group vars.

- name: Authenticate with Docker Hub
  community.docker.docker_login:
    username: "{{ dockerhub_username }}"
    password: "{{ dockerhub_password }}"
    reauthorize: true

- name: Ensure Docker network exists
  community.docker.docker_network:
    name: "{{ docker_network_name }}"
    state: present

- name: Ensure persistent volumes exist
  community.docker.docker_volume:
    name: "{{ item }}"
    state: present
  loop:
    - "{{ postgres_volume_name }}"
    - "{{ rabbitmq_volume_name }}"
    - "{{ loki_volume_name }}"
    - "{{ prometheus_volume_name }}"
    - "{{ grafana_volume_name }}"

- name: Prepare list of TCP ports to open
  ansible.builtin.set_fact:
    banking_firewall_ports: "{{ firewall_tcp_ports | map('string') | list | unique }}"
  when: firewall_tcp_ports | length > 0

- name: Check if UFW is available
  ansible.builtin.stat:
    path: /usr/sbin/ufw
  register: ufw_binary
  when: banking_firewall_ports is defined and banking_firewall_ports | length > 0

- name: Allow required ports via UFW
  community.general.ufw:
    rule: allow
    port: "{{ item }}"
    proto: tcp
  loop: "{{ banking_firewall_ports }}"
  when:
    - banking_firewall_ports is defined
    - banking_firewall_ports | length > 0
    - ufw_binary is defined
    - ufw_binary.stat.exists

- name: Check if firewalld is running
  ansible.builtin.command: firewall-cmd --state
  register: firewalld_state
  changed_when: false
  failed_when: false
  when: banking_firewall_ports is defined and banking_firewall_ports | length > 0

- name: Ensure required ports are open in firewalld
  ansible.posix.firewalld:
    port: "{{ item }}/tcp"
    permanent: true
    state: enabled
    immediate: true
  loop: "{{ banking_firewall_ports }}"
  when:
    - banking_firewall_ports is defined
    - banking_firewall_ports | length > 0
    - firewalld_state is defined
    - firewalld_state.rc == 0

- name: Ensure release metadata directory exists
  ansible.builtin.file:
    path: "{{ stable_tag_file | dirname }}"
    state: directory
    mode: '0755'

- name: Check for existing stable tag record
  ansible.builtin.stat:
    path: "{{ stable_tag_file }}"
  register: stable_tag_file_stat

- name: Read stored stable tag
  ansible.builtin.slurp:
    path: "{{ stable_tag_file }}"
  register: stored_stable_tag_raw
  when: stable_tag_file_stat.stat.exists

- name: Determine stored stable tag value
  ansible.builtin.set_fact:
    stored_stable_tag: "{{ stored_stable_tag_raw.content | b64decode | trim }}"
  when:
    - stored_stable_tag_raw is defined
    - stored_stable_tag_raw.content is defined

- name: Calculate current stable tag
  ansible.builtin.set_fact:
    current_stable_tag: >-
      {{ (stable_tag_override | length > 0)
           | ternary(stable_tag_override, (stored_stable_tag | default(stable_tag_default))) }}

- name: Determine effective canary tag
  ansible.builtin.set_fact:
    effective_canary_tag: "{{ (canary_tag | length > 0) | ternary(canary_tag, current_stable_tag) }}"

- name: Promote canary to stable when requested
  ansible.builtin.set_fact:
    current_stable_tag: "{{ effective_canary_tag }}"
  when: promote_canary | bool

- name: Persist stable tag selection
  ansible.builtin.copy:
    content: "{{ current_stable_tag }}\n"
    dest: "{{ stable_tag_file }}"
    mode: '0644'

- name: Define image references for stable and canary deployments
  ansible.builtin.set_fact:
    account_service_stable_image: "{{ account_service_image_repo }}:{{ current_stable_tag }}"
    account_service_canary_image: "{{ account_service_image_repo }}:{{ effective_canary_tag }}"
    transaction_service_stable_image: "{{ transaction_service_image_repo }}:{{ current_stable_tag }}"
    transaction_service_canary_image: "{{ transaction_service_image_repo }}:{{ effective_canary_tag }}"

- name: Define shared environment configuration
  ansible.builtin.set_fact:
    service_common_env:
      DATABASE_URL: "postgresql://{{ postgres_user | urlencode }}:{{ postgres_password | urlencode }}@{{ postgres_container_name }}:5432/{{ postgres_db }}"
      RABBITMQ_HOST: "{{ rabbitmq_container_name }}"
      RABBITMQ_PORT: "{{ rabbitmq_port }}"
      RABBITMQ_USER: "{{ rabbitmq_user }}"
      RABBITMQ_PASSWORD: "{{ rabbitmq_password }}"
      RABBITMQ_QUEUE: "{{ rabbitmq_queue }}"

- name: Initialize canary routing metadata
  ansible.builtin.set_fact:
    banking_canary_enabled: "{{ enable_canary | bool }}"
    banking_canary_weight: "{{ (enable_canary | bool) | ternary(canary_traffic_percentage | int, 0) }}"
    canary_failure_flag: false

- name: Share canary metadata with gateway role
  ansible.builtin.set_fact:
    banking_account_stable_name: "{{ account_service_stable_container_name }}"
    banking_account_canary_name: "{{ account_service_canary_container_name }}"
    banking_transaction_stable_name: "{{ transaction_service_stable_container_name }}"
    banking_transaction_canary_name: "{{ transaction_service_canary_container_name }}"
    banking_account_container_port: "{{ account_service_container_port }}"
    banking_transaction_container_port: "{{ transaction_service_container_port }}"
    banking_stable_tag: "{{ current_stable_tag }}"
    banking_canary_tag: "{{ effective_canary_tag }}"

- name: Ensure canary failure log directory exists
  ansible.builtin.file:
    path: "{{ canary_failure_log_dir }}"
    state: directory
    mode: '0755'

- name: Create monitoring configuration directory
  ansible.builtin.file:
    path: "{{ monitoring_config_path }}"
    state: directory
    mode: '0755'

- name: Copy Loki configuration file
  ansible.builtin.copy:
    src: "{{ playbook_dir }}/../../monitoring/loki-config.yml"
    dest: "{{ loki_config_file }}"
    mode: '0644'
    remote_src: false

- name: Copy Promtail configuration file
  ansible.builtin.copy:
    src: "{{ playbook_dir }}/../../monitoring/promtail-config.yml"
    dest: "{{ promtail_config_file }}"
    mode: '0644'
    remote_src: false

- name: Copy Prometheus configuration file
  ansible.builtin.copy:
    src: "{{ playbook_dir }}/../../monitoring/prometheus-config.yml"
    dest: "{{ prometheus_config_file }}"
    mode: '0644'
    remote_src: false

- name: Copy Grafana datasources configuration file
  ansible.builtin.copy:
    src: "{{ playbook_dir }}/../../monitoring/grafana-datasources.yml"
    dest: "{{ grafana_datasources_file }}"
    mode: '0644'
    remote_src: false

- name: Copy Grafana dashboard provisioning configuration file
  ansible.builtin.copy:
    src: "{{ playbook_dir }}/../../monitoring/grafana-dashboard-provisioning.yml"
    dest: "{{ grafana_dashboard_provisioning_file }}"
    mode: '0644'
    remote_src: false

- name: Create Grafana dashboards directory
  ansible.builtin.file:
    path: "{{ grafana_dashboards_dir }}"
    state: directory
    mode: '0755'

- name: Copy Grafana dashboards
  ansible.builtin.copy:
    src: "{{ item }}"
    dest: "{{ grafana_dashboards_dir }}/{{ item | basename }}"
    mode: '0644'
    remote_src: false
  loop: "{{ lookup('fileglob', playbook_dir + '/../../monitoring/grafana-dashboards/*.json', wantlist=True) }}"

- name: Run Postgres database container
  community.docker.docker_container:
    name: "{{ postgres_container_name }}"
    image: "{{ postgres_image }}"
    restart_policy: unless-stopped
    state: started
    env:
      POSTGRES_USER: "{{ postgres_user }}"
      POSTGRES_PASSWORD: "{{ postgres_password }}"
      POSTGRES_DB: "{{ postgres_db }}"
    volumes:
      - "{{ postgres_volume_name }}:/var/lib/postgresql/data"
    networks:
      - name: "{{ docker_network_name }}"
    labels:
      service: "postgres"
      logging: "true"
    log_driver: json-file
    log_options:
      max-size: "10m"
      max-file: "3"
      labels: "service"

- name: Run RabbitMQ container
  community.docker.docker_container:
    name: "{{ rabbitmq_container_name }}"
    image: "{{ rabbitmq_image }}"
    restart_policy: unless-stopped
    state: started
    command: >
      sh -c "rabbitmq-plugins enable --offline rabbitmq_prometheus &&
             rabbitmq-server"
    env:
      RABBITMQ_DEFAULT_USER: "{{ rabbitmq_user }}"
      RABBITMQ_DEFAULT_PASS: "{{ rabbitmq_password }}"
    volumes:
      - "{{ rabbitmq_volume_name }}:/var/lib/rabbitmq"
    networks:
      - name: "{{ docker_network_name }}"
    published_ports:
      - "{{ rabbitmq_port }}:{{ rabbitmq_port }}"
      - "{{ rabbitmq_management_port }}:{{ rabbitmq_management_port }}"
      - "{{ rabbitmq_metrics_port }}:{{ rabbitmq_metrics_port }}"
    labels:
      service: "rabbitmq"
      logging: "true"
    log_driver: json-file
    log_options:
      max-size: "10m"
      max-file: "3"
      labels: "service"


- name: Run account service canary release
  community.docker.docker_container:
    name: "{{ account_service_canary_container_name }}"
    image: "{{ account_service_canary_image }}"
    restart_policy: unless-stopped
    state: started
    pull: true
    recreate: true
    env: "{{ service_common_env }}"
    networks:
      - name: "{{ docker_network_name }}"
    published_ports:
      - "{{ account_service_canary_port }}:{{ account_service_container_port }}"
    labels:
      logging: "true"
      service: "account-service-canary"
    log_driver: json-file
    log_options:
      max-size: "10m"
      max-file: "3"
      labels: "service,logging"
  when: enable_canary | bool


- name: Run transaction service canary release
  community.docker.docker_container:
    name: "{{ transaction_service_canary_container_name }}"
    image: "{{ transaction_service_canary_image }}"
    restart_policy: unless-stopped
    state: started
    pull: true
    recreate: true
    env: "{{ service_common_env }}"
    networks:
      - name: "{{ docker_network_name }}"
    published_ports:
      - "{{ transaction_service_canary_port }}:{{ transaction_service_container_port }}"
    labels:
      logging: "true"
      service: "transaction-service-canary"
    log_driver: json-file
    log_options:
      max-size: "10m"
      max-file: "3"
      labels: "service,logging"
  when: enable_canary | bool

- name: Probe canary account service health
  ansible.builtin.uri:
    url: "http://127.0.0.1:{{ account_service_canary_port }}/health"
    method: GET
    status_code: 200
  register: account_canary_health
  retries: 5
  delay: 3
  failed_when: false
  when: enable_canary | bool

- name: Probe canary transaction service health
  ansible.builtin.uri:
    url: "http://127.0.0.1:{{ transaction_service_canary_port }}/health"
    method: GET
    status_code: 200
  register: transaction_canary_health
  retries: 5
  delay: 3
  failed_when: false
  when: enable_canary | bool

- name: Evaluate canary health status
  ansible.builtin.set_fact:
    canary_failure_flag: "{{ canary_failure_flag or ((account_canary_health.status | default(0)) != 200 or (transaction_canary_health.status | default(0)) != 200) }}"
  when: enable_canary | bool

- name: Record canary failure timestamp
  ansible.builtin.set_fact:
    canary_failure_timestamp: "{{ ansible_date_time.iso8601 }}"
  when: canary_failure_flag

- name: Capture canary container logs when unhealthy
  when: canary_failure_flag
  block:
    - name: Collect logs from canary containers
      ansible.builtin.command: "docker logs --tail 400 {{ item }}"
      register: canary_failure_logs
      changed_when: false
      loop:
        - "{{ account_service_canary_container_name }}"
        - "{{ transaction_service_canary_container_name }}"

    - name: Persist canary failure logs
      ansible.builtin.copy:
        content: "{{ item.stdout }}"
        dest: "{{ canary_failure_log_dir }}/{{ item.item }}-{{ canary_failure_timestamp | regex_replace(':', '') }}.log"
        mode: '0644'
      loop: "{{ canary_failure_logs.results }}"

    - name: Write canary failure summary
      ansible.builtin.copy:
        content: |
          {
            "timestamp": "{{ canary_failure_timestamp }}",
            "account_status": "{{ account_canary_health.status | default('unavailable') }}",
            "transaction_status": "{{ transaction_canary_health.status | default('unavailable') }}"
          }
        dest: "{{ canary_failure_log_dir }}/summary-{{ canary_failure_timestamp | regex_replace(':', '') }}.json"
        mode: '0644'

- name: Disable canary routing after failure
  ansible.builtin.set_fact:
    enable_canary: false
    banking_canary_enabled: false
    banking_canary_weight: 0
    canary_traffic_percentage: 0
  when: canary_failure_flag

- name: Apply routing changes after canary failure
  ansible.builtin.include_role:
    name: nginx_canary
  when: canary_failure_flag

- name: Temporarily route traffic through canary during promotion
  block:
    - name: Increase canary traffic weight to 100 percent
      ansible.builtin.set_fact:
        banking_canary_weight: 100

    - name: Apply temporary full-canary routing
      ansible.builtin.include_role:
        name: nginx_canary
  when:
    - promote_canary | bool
    - banking_canary_enabled
    - not canary_failure_flag

- name: Run account service (stable release)
  community.docker.docker_container:
    name: "{{ account_service_stable_container_name }}"
    image: "{{ account_service_stable_image }}"
    restart_policy: unless-stopped
    state: started
    pull: true
    recreate: true
    env: "{{ service_common_env }}"
    networks:
      - name: "{{ docker_network_name }}"
    published_ports:
      - "{{ account_service_port }}:{{ account_service_container_port }}"
    labels:
      logging: "true"
      service: "account-service-stable"
    log_driver: json-file
    log_options:
      max-size: "10m"
      max-file: "3"
      labels: "service,logging"
  when: not canary_failure_flag

- name: Run transaction service (stable release)
  community.docker.docker_container:
    name: "{{ transaction_service_stable_container_name }}"
    image: "{{ transaction_service_stable_image }}"
    restart_policy: unless-stopped
    state: started
    pull: true
    recreate: true
    env: "{{ service_common_env }}"
    networks:
      - name: "{{ docker_network_name }}"
    published_ports:
      - "{{ transaction_service_port }}:{{ transaction_service_container_port }}"
    labels:
      logging: "true"
      service: "transaction-service-stable"
    log_driver: json-file
    log_options:
      max-size: "10m"
      max-file: "3"
      labels: "service,logging"
  when: not canary_failure_flag

- name: Remove canary containers when disabled
  community.docker.docker_container:
    name: "{{ item }}"
    state: absent
  loop:
    - "{{ account_service_canary_container_name }}"
    - "{{ transaction_service_canary_container_name }}"
  when: not (enable_canary | bool)

- name: Fail deployment because canary health checks failed
  ansible.builtin.fail:
    msg: "Canary deployment failed health checks. See {{ canary_failure_log_dir }} for details."
  when: canary_failure_flag

- name: Run Loki container
  community.docker.docker_container:
    name: "{{ loki_container_name }}"
    image: "{{ loki_image }}"
    restart_policy: unless-stopped
    state: started
    pull: true
    recreate: true
    command: -config.file=/etc/loki/loki-config.yaml
    volumes:
      - "{{ loki_volume_name }}:/loki"
      - "{{ loki_config_file }}:/etc/loki/loki-config.yaml:ro"
    networks:
      - name: "{{ docker_network_name }}"
    published_ports:
      - "{{ loki_port }}:3100"
    labels:
      service: "loki"
      component: "monitoring"

- name: Wait for Loki to be ready
  ansible.builtin.uri:
    url: "http://localhost:{{ loki_port }}/ready"
    method: GET
    status_code: [200, 404]
  register: loki_ready
  until: loki_ready.status == 200
  retries: 30
  delay: 2
  ignore_errors: true

- name: Run Promtail container
  community.docker.docker_container:
    name: "{{ promtail_container_name }}"
    image: "{{ promtail_image }}"
    restart_policy: unless-stopped
    state: started
    pull: true
    recreate: true
    command: -config.file=/etc/promtail/config.yml
    volumes:
      - "/var/lib/docker/containers:/var/lib/docker/containers:ro"
      - "/var/run/docker.sock:/var/run/docker.sock:ro"
      - "{{ promtail_config_file }}:/etc/promtail/config.yml:ro"
    networks:
      - name: "{{ docker_network_name }}"
    labels:
      service: "promtail"
      component: "monitoring"

- name: Run Prometheus container
  community.docker.docker_container:
    name: "{{ prometheus_container_name }}"
    image: "{{ prometheus_image }}"
    restart_policy: unless-stopped
    state: started
    pull: true
    recreate: true
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--storage.tsdb.retention.time=30d'
    volumes:
      - "{{ prometheus_volume_name }}:/prometheus"
      - "{{ prometheus_config_file }}:/etc/prometheus/prometheus.yml:ro"
    networks:
      - name: "{{ docker_network_name }}"
    published_ports:
      - "{{ prometheus_port }}:9090"
    labels:
      service: "prometheus"
      component: "monitoring"

- name: Wait for Prometheus to be ready
  ansible.builtin.uri:
    url: "http://localhost:{{ prometheus_port }}/-/ready"
    method: GET
    status_code: [200, 404]
  register: prometheus_ready
  until: prometheus_ready.status == 200
  retries: 30
  delay: 2
  ignore_errors: true

- name: Run Grafana container
  community.docker.docker_container:
    name: "{{ grafana_container_name }}"
    image: "{{ grafana_image }}"
    restart_policy: unless-stopped
    state: started
    pull: true
    recreate: true
    env:
      GF_SECURITY_ADMIN_USER: "{{ grafana_admin_user }}"
      GF_SECURITY_ADMIN_PASSWORD: "{{ grafana_admin_password }}"
      GF_USERS_ALLOW_SIGN_UP: "{{ grafana_allow_sign_up }}"
    volumes:
      - "{{ grafana_volume_name }}:/var/lib/grafana"
      - "{{ grafana_datasources_file }}:/etc/grafana/provisioning/datasources/datasources.yml:ro"
      - "{{ grafana_dashboard_provisioning_file }}:/etc/grafana/provisioning/dashboards/default.yml:ro"
      - "{{ grafana_dashboards_dir }}:/etc/grafana/provisioning/dashboards/dashboards:ro"
    networks:
      - name: "{{ docker_network_name }}"
    published_ports:
      - "{{ grafana_port }}:3000"
    labels:
      service: "grafana"
      component: "monitoring"
- name: Verify promoted account service via host port
  ansible.builtin.uri:
    url: "http://127.0.0.1:{{ account_service_port }}/health"
    method: GET
    status_code: 200
  register: account_stable_health
  retries: 5
  delay: 3
  when:
    - not canary_failure_flag
    - promote_canary | bool

- name: Verify promoted transaction service via host port
  ansible.builtin.uri:
    url: "http://127.0.0.1:{{ transaction_service_port }}/health"
    method: GET
    status_code: 200
  register: transaction_stable_health
  retries: 5
  delay: 3
  when:
    - not canary_failure_flag
    - promote_canary | bool

- name: Finalize canary promotion
  block:
    - name: Disable canary slot after promotion
      ansible.builtin.set_fact:
        enable_canary: false
        banking_canary_enabled: false
        banking_canary_weight: 0

    - name: Restore stable-only routing
      ansible.builtin.include_role:
        name: nginx_canary
  when:
    - promote_canary | bool
    - not canary_failure_flag
